{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax 101 Exercise Solutions\n",
    "---\n",
    "This notebook accompanies the `Jax 101` [blog post](link) and the `exercises.ipynb` notebook. Make sure to read that post and complete the exercises before looking at this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, grad, vmap\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 1: Array Creation\n",
    "\n",
    "- **Objective:** Learn basic array creation techniques.\n",
    "- **Tasks:**\n",
    "  - Create an array of values that range from 0 to 100.\n",
    "  - Create an matrix of zeros of size (100 x 1000) with dtype jnp.float64.\n",
    "  - Use `jnp.polyval` to create an array of values. Visualize the result using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = jnp.arange(100)\n",
    "arr2 = jnp.zeros((100, 1000), dtype=jnp.float64)\n",
    "print(arr[:10])\n",
    "print(arr.dtype, arr2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.polyval(jnp.array([-3, 4, -2, 7]), jnp.linspace(-5, 5, 100))\n",
    "plt.plot(jnp.arange(a.shape[0]), a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Higher Order Grads\n",
    "\n",
    "- **Objective:** Learn about `grad`.\n",
    "- **Tasks:**\n",
    "  - Write the function defined below.\n",
    "  - Create an array of values that range from 0 to 100.\n",
    "  - Using `grad` calculate the 1st, 2nd, 3rd, and 4th order gradients of the function\n",
    "    - Hint: You can nest `grad` calls like `grad(grad(f))`\n",
    "  - Plot the resulting arrays on a single figure\n",
    "\n",
    "$\n",
    "f(x) = x^3 - 3x^2 + 2x\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3 - 3*x**2 + 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.arange(7.0)\n",
    "\n",
    "print(f\"original:           {x} --> {f(x)}\")\n",
    "print(f\"grad:               {x} --> {vmap(grad(f))(x)}\")\n",
    "print(f\"2nd order grad:     {x} --> {vmap(grad(grad(f)))(x)}\")\n",
    "print(f\"3rd order grad:     {x} --> {vmap(grad(grad(grad(f))))(x)}\")\n",
    "print(f\"4th order grad:     {x} --> {vmap(grad(grad(grad(grad(f)))))(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.linspace(-10, 10, 2_000)\n",
    "\n",
    "plt.plot(xs, f(xs), label=\"f(x)\")\n",
    "plt.plot(xs, vmap(grad(f))(xs), label=\"1st order grad\")\n",
    "plt.plot(xs, vmap(grad(grad(f)))(xs), label=\"2nd order grad\")\n",
    "plt.plot(xs, vmap(grad(grad(grad(f))))(xs), label=\"3rd order grad\")\n",
    "plt.grid(True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Random Numbers\n",
    "\n",
    "- **Objective:** Learn about Jax's random module.\n",
    "- **Tasks:**\n",
    "  - Use `random.key` to create a key and then create an array of normally distributed numbers.\n",
    "  - What happens if you try to reuse the key?\n",
    "  - Bonus points: Implement a Monte Carlo simulation to estimate the value of Ï€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.key(0)\n",
    "arr = random.normal(key, 10)\n",
    "arr_reused = random.normal(key, 10)\n",
    "print(arr)\n",
    "print(arr_reused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.key(0)\n",
    "arr = random.uniform(key, (10_000, 2), minval=0, maxval=1)\n",
    "\n",
    "invals = (jnp.linalg.norm(arr, axis=1) < 1).sum()\n",
    "pi = invals / arr.shape[0] * 4\n",
    "print(f\"Estimated value of pi: {pi:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Jit\n",
    "\n",
    "- **Objective:** Understand and apply Just-In-Time (JIT) compilation for performance optimization.\n",
    "- **Tasks:**\n",
    "  - Implement the function described below.\n",
    "  - Use `jax.jit` to optimize the function.\n",
    "  - Measure and compare the execution time before and after applying `jax.jit`.\n",
    "    - Hint: For accurate timing make sure to use `block_until_ready`.\n",
    "  - Experiment with `static_argnums` and `donate_argnums` to understand their impact on performance.\n",
    "\n",
    "$\n",
    "f(x) = x^2 + 2x + 1\n",
    "$\n",
    "\n",
    "**Best Practice**: Apply `vmap` and then `jit` so that you optimize the vectorized version of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 2*x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jitted = jit(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.key(0)\n",
    "\n",
    "arr = random.normal(key, (1_000_000, 100))\n",
    "\n",
    "_ = f_jitted(arr) # compile function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit f(arr).block_until_ready()\n",
    "%timeit f_jitted(arr).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static argnums\n",
    "def f(x, y):\n",
    "    return x**2 + x**0.5 - y**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cached code gets used when the same 0-th argument gets used. otherwise recompilation occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jitted = jit(f)\n",
    "f_staticargnum_jitted = jit(f, static_argnums=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data and compile function\n",
    "key = random.key(0)\n",
    "\n",
    "x = random.normal(key, (1_000_000, 100))\n",
    "y = random.normal(key, (1_000_000, 100))\n",
    "xnew = 2.0\n",
    "ynew = jnp.array(3.0)\n",
    "\n",
    "\n",
    "_ = f_jitted(x, y) # compile function\n",
    "_ = f_staticargnum_jitted(xnew, ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 f(x, y).block_until_ready()\n",
    "%timeit -n 10 f_jitted(x, y).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Intermediate Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Vectorization with `vmap`\n",
    "- **Objective:** Learn how to efficiently vectorize operations using `jax.vmap`.\n",
    "- **Tasks:**\n",
    "  - Implement a function that computes the dot product of two vectors.\n",
    "  - Use `jax.vmap` to apply this function across a batch of vectors.\n",
    "  - Extend the function to compute the matrix-vector product for a batch of matrices and vectors.\n",
    "  - Compare the performance of the vectorized version with a loop-based implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return jnp.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.arange(10)\n",
    "y = jnp.arange(10) + 5\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vmapped = vmap(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for timing test\n",
    "key = random.key(0)\n",
    "xkey, ykey = random.split(key)\n",
    "x = random.normal(xkey, (10_000, 100))\n",
    "y = random.normal(ykey, (10_000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit [f(i, j) for i, j in zip(x, y)]\n",
    "%timeit f_vmapped(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Working with Custom Gradients\n",
    "- **Objective:** Implement custom gradients for non-standard operations.\n",
    "- **Tasks:**\n",
    "  - Implement the relu function.\n",
    "  - Define a custom gradient for this function using `jax.custom_jvp` or `jax.custom_vjp`.\n",
    "  - Implement and test a function that uses this custom gradient in an optimization problem.\n",
    "  - Plot the results and use Jax's built-in `grad` and `jax.nn.relu` to confirm the outputs match your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_vjp\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def relu_fwd(x):\n",
    "    y = relu(x)\n",
    "    return y, x\n",
    "\n",
    "def relu_bwd(res, g):\n",
    "    y = res\n",
    "    grad_x = jnp.where(y > 0., 1., 0.) * g\n",
    "    return (grad_x,)\n",
    "\n",
    "relu.defvjp(relu_fwd, relu_bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.linspace(-10, 10, 2_000)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(xs, relu(xs), label=\"relu\")\n",
    "ax.plot(xs, vmap(grad(relu))(xs), label=\"grad(relu)\")\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.grid(True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Neural Networks with Jax\n",
    "- **Objective:** Build and train a simple neural network from scratch using Jax.\n",
    "- **Tasks:**\n",
    "  - Implement a basic feedforward neural network for a classification task.\n",
    "  - Use `jax.numpy` for all operations, and manually implement forward and backward passes.\n",
    "  - Implement training using stochastic gradient descent.\n",
    "  - Experiment with different activation functions and regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data for binary classification task\n",
    "# Parameters for the random data\n",
    "n = 50\n",
    "num_classes = 2\n",
    "\n",
    "# Generate random data points\n",
    "key = random.key(0)\n",
    "xkey0, xkey1 = random.split(key, 2)\n",
    "\n",
    "mean_class_0 = jnp.array([-2, -2])  # mean for class 0\n",
    "mean_class_1 = jnp.array([2, 2])  # mean for class 1\n",
    "std_dev = 1.0  # standard deviation for both classes\n",
    "\n",
    "# Generate data for class 0\n",
    "X_class_0 = random.normal(xkey0, (n, 2)) * std_dev + mean_class_0\n",
    "y_class_0 = jnp.zeros(n)  # label 0 for class 0\n",
    "\n",
    "# Generate data for class 1\n",
    "X_class_1 = random.normal(xkey1, (n, 2)) * std_dev + mean_class_1\n",
    "y_class_1 = jnp.ones(n)  # label 1 for class 1\n",
    "\n",
    "# Assign random class labels (0 or 1)\n",
    "# y = random.randint(ykey, n, 0, num_classes)\n",
    "X = jnp.vstack((X_class_0, X_class_1))\n",
    "y = jnp.expand_dims(jnp.hstack((y_class_0, y_class_1)), -1)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Visualize the generated data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Randomly Generated Data for Binary Classification\")\n",
    "plt.show()\n",
    "\n",
    "# Show the first few rows of the generated data\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network weights\n",
    "Wkey1, Wkey2 = random.split(key, 2)\n",
    "W1 = random.normal(Wkey1, (n*2, 32)) * 1e-2\n",
    "b1 = jnp.ones((32,1))\n",
    "\n",
    "W2 = random.normal(Wkey2, (32, n*2)) * 1e-2\n",
    "b2 = jnp.ones((n*2,1))\n",
    "\n",
    "params = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train via gradient descent\n",
    "def predict(params, x):\n",
    "    x_h = jax.nn.relu(params[\"W1\"].T @ x + params[\"b1\"])\n",
    "    preds = jax.nn.sigmoid(params[\"W2\"].T @ x_h + params[\"b2\"])\n",
    "    return preds\n",
    "\n",
    "# @jit\n",
    "def loss_fn(params, x, y):\n",
    "    preds = predict(params, x)\n",
    "    return jnp.square(preds - y).mean()\n",
    "\n",
    "steps = 2000\n",
    "lr = 0.01\n",
    "losses = jnp.empty((steps,))\n",
    "predictions = []\n",
    "for i in range(steps):\n",
    "    preds = predict(params, X)\n",
    "    if (i % 10) == 0:\n",
    "        predictions.append(preds)\n",
    "    # calculate loss + grad\n",
    "    loss_value, grad_value = jax.value_and_grad(loss_fn)(params, preds, y)\n",
    "    losses = losses.at[i].set(loss_value)\n",
    "    # update weights\n",
    "    for param in params:\n",
    "        params[param] += -grad_value[param]*lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label=\"train loss\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See predictions made by trained model\n",
    "preds = predict(params, X)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(preds[:, 0], preds[:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Final Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gif of predictions over time (optional)\n",
    "frames = []\n",
    "\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    # Create a plot for each frame\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(predictions[i][:, 0], predictions[i][:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(f\"Predictions at step {i+1}/{len(predictions)}\")\n",
    "    \n",
    "    # Save the plot to a BytesIO object\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Add this frame to the list\n",
    "    frames.append(imageio.v3.imread(buf))\n",
    "\n",
    "# Save the frames as a GIF\n",
    "imageio.mimsave('ex7_sgd.gif', frames, fps=2)  # You can adjust fps for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to display the GIF in a Jupyter notebook (optional):\n",
    "from IPython.display import Image\n",
    "Image(filename='ex7_sgd.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Parallelism with `pmap`\n",
    "- **Objective:** Leverage data parallelism to scale computations across multiple devices.\n",
    "- **Tasks:**\n",
    "  - Implement a function to compute across a large array. It should be complicated enough to notice a difference between a jitted version and a non-jitted version\n",
    "  - Use `jax.pmap` to parallelize this operation across multiple devices (e.g., GPUs).\n",
    "  - Measure the speedup obtained with `pmap` compared to single-device execution.\n",
    "  - Experiment with different batch sizes and data partitioning strategies.\n",
    "  - Try doing this in [Google Colab](https://colab.research.google.com) for free and easy access to multiple accelerator devices.\n",
    "  \n",
    "Note: `jax.pmap` compiles the function so `jit()` is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.local_device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return jnp.sum(5*x - y**2 + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.key(0)\n",
    "key1, key2 = random.split(key, 2)\n",
    "\n",
    "x1 = random.normal(key1, (8, 100_000))\n",
    "x2 = random.normal(key2, (8, 100_000))\n",
    "f(x1, x2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this on machine with hardware accelerator. It will not work on CPU\n",
    "# jitted_f = jax.pmap(f)\n",
    "# _ = jitted_f(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit f(x1, x2)\n",
    "# %timeit jitted_f(x1, x2).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: JaxPR\n",
    "- **Objective:** Dive into Jax's internal representation by manipulating JaxPR.\n",
    "- **Tasks:**\n",
    "  - Define a function and obtain it's jaxpr using `jax.make_jaxpr`.\n",
    "  - Analyze the jaxpr to understand it's computation graph. Compare it with the one from the blog post.\n",
    "  - Modify the function and observe how the jaxpr changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = x + 3\n",
    "    x = x ** 2\n",
    "    y = 21\n",
    "    return jnp.sum(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(f)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
