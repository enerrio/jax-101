{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax 101 Exercises\n",
    "---\n",
    "This notebook accompanies the `Jax 101` [blog post](link). Make sure to check that out before diving into these exercises. The post will not cover everything you need to complete these problems so make sure to refer back to the [Jax documentation](https://jax.readthedocs.io/en/latest/index.html) if you get stuck. \n",
    "\n",
    "Try to make use of Jax's debugging tools like `jax.debug.print`, you will not be able to print array values inside a compiled function. Also for timing use Python's built-in `timeit` library. You can do this easily inside a Jupyter notebook like so:\n",
    "```\n",
    "%timeit myfunction(x, y)\n",
    "```\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 1: Array Creation\n",
    "\n",
    "- **Objective:** Learn basic array creation techniques.\n",
    "- **Tasks:**\n",
    "  - Create an array of values that range from 0 to 100.\n",
    "  - Create an matrix of zeros of size (100 x 1000) with dtype jnp.float64.\n",
    "  - Use `jnp.polyval` to create an array of values. Visualize the result using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Higher Order Grads\n",
    "\n",
    "- **Objective:** Learn about `grad`.\n",
    "- **Tasks:**\n",
    "  - Write the function defined below.\n",
    "  - Create an array of values that range from -10 to +10.\n",
    "  - Using `grad` calculate the 1st, 2nd, 3rd, and 4th order gradients of the function\n",
    "    - Hint: You can nest `grad` calls like `grad(grad(f))`\n",
    "  - Plot the resulting arrays on a single figure\n",
    "\n",
    "$\n",
    "f(x) = x^3 - 3x^2 + 2x\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ### TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Random Numbers\n",
    "\n",
    "- **Objective:** Learn about Jax's random module.\n",
    "- **Tasks:**\n",
    "  - Use `random.key` to create a key and then create an array of normally distributed numbers.\n",
    "  - What happens if you try to reuse the key?\n",
    "  - Bonus points: Implement a Monte Carlo simulation to estimate the value of Ï€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Jit\n",
    "\n",
    "- **Objective:** Understand and apply Just-In-Time (JIT) compilation for performance optimization.\n",
    "- **Tasks:**\n",
    "  - Implement the function described below.\n",
    "  - Use `jax.jit` to optimize the function.\n",
    "  - Measure and compare the execution time before and after applying `jax.jit`.\n",
    "    - Hint: For accurate timing make sure to use `block_until_ready`.\n",
    "  - Experiment with `static_argnums` and `donate_argnums` to understand their impact on performance.\n",
    "\n",
    "$\n",
    "f(x) = x^2 + 2x + 1\n",
    "$\n",
    "\n",
    "**Best Practice**: Apply `vmap` and then `jit` so that you optimize the vectorized version of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ### TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Intermediate Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Vectorization with `vmap`\n",
    "- **Objective:** Learn how to efficiently vectorize operations using `jax.vmap`.\n",
    "- **Tasks:**\n",
    "  - Implement a function that computes the dot product of two vectors.\n",
    "  - Use `jax.vmap` to apply this function across a batch of vectors.\n",
    "  - Extend the function to compute the matrix-vector product for a batch of matrices and vectors.\n",
    "  - Compare the performance of the vectorized version with a loop-based implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    ### TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_vmapped = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit [f(i, j) for i, j in zip(x, y)]\n",
    "%timeit f_vmapped(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Working with Custom Gradients\n",
    "- **Objective:** Implement custom gradients for non-standard operations.\n",
    "- **Tasks:**\n",
    "  - Implement the relu function.\n",
    "  - Define a custom gradient for this function using `jax.custom_jvp` or `jax.custom_vjp`.\n",
    "  - Plot the results and use Jax's built-in `grad` and `jax.nn.relu` to confirm the outputs match your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    ### TODO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.linspace(-10, 10, 2_000)\n",
    "ys_relu = relu(xs)\n",
    "# ys_grad = ...\n",
    "\n",
    "plt.plot(xs, ys_relu, label=\"relu\")\n",
    "plt.plot(xs, ys_grad, label=\"grad(relu)\")\n",
    "plt.grid(True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Neural Networks with Jax\n",
    "- **Objective:** Build and train a simple neural network from scratch using Jax.\n",
    "- **Tasks:**\n",
    "  - Implement a basic feedforward neural network for a classification task.\n",
    "  - Use `jax.numpy` for all operations, and manually implement forward and backward passes.\n",
    "  - Implement training using stochastic gradient descent.\n",
    "  - Experiment with different activation functions and regularization techniques.\n",
    "  - Try jitting the train loop and see if training time goes down.\n",
    "  - Think about why we have to pass in `params` to each function. Why not access it in the global scope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data for binary classification task\n",
    "# Parameters for the random data\n",
    "n = 50\n",
    "num_classes = 2\n",
    "\n",
    "# Generate random data points\n",
    "key = random.key(0)\n",
    "xkey0, xkey1 = random.split(key, 2)\n",
    "\n",
    "mean_class_0 = jnp.array([-2, -2])  # mean for class 0\n",
    "mean_class_1 = jnp.array([2, 2])  # mean for class 1\n",
    "std_dev = 1.0  # standard deviation for both classes\n",
    "\n",
    "# Generate data for class 0\n",
    "X_class_0 = random.normal(xkey0, (n, 2)) * std_dev + mean_class_0\n",
    "y_class_0 = jnp.zeros(n)  # label 0 for class 0\n",
    "\n",
    "# Generate data for class 1\n",
    "X_class_1 = random.normal(xkey1, (n, 2)) * std_dev + mean_class_1\n",
    "y_class_1 = jnp.ones(n)  # label 1 for class 1\n",
    "\n",
    "# Assign random class labels (0 or 1)\n",
    "# y = random.randint(ykey, n, 0, num_classes)\n",
    "X = jnp.vstack((X_class_0, X_class_1))\n",
    "y = jnp.expand_dims(jnp.hstack((y_class_0, y_class_1)), -1)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Visualize the generated data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Randomly Generated Data for Binary Classification\")\n",
    "plt.show()\n",
    "\n",
    "# Show the first few rows of the generated data\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network weights\n",
    "### TODO ###\n",
    "# params = {\"W1\": ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train via gradient descent\n",
    "def predict(params, x):\n",
    "    ### TODO ###\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    ### TODO ###\n",
    "\n",
    "steps = 2000\n",
    "lr = 0.01\n",
    "losses = jnp.empty((steps,)) # Save loss values here\n",
    "predictions = []\n",
    "for i in range(steps):\n",
    "    preds = predict(params, X)\n",
    "    ### TODO ###\n",
    "    # Save predictions for visualization later (optional)\n",
    "    ### TODO ###\n",
    "    # loss_value, grad_value = ...\n",
    "    ### TODO ###\n",
    "    # update weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label=\"train loss\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See predictions made by trained model\n",
    "preds = predict(params, X)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(preds[:, 0], preds[:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Final Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gif of predictions over time (optional)\n",
    "frames = []\n",
    "\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    # Create a plot for each frame\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(predictions[i][:, 0], predictions[i][:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(f\"Predictions at step {i+1}/{len(predictions)}\")\n",
    "    \n",
    "    # Save the plot to a BytesIO object\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Add this frame to the list\n",
    "    frames.append(imageio.v3.imread(buf))\n",
    "\n",
    "# Save the frames as a GIF\n",
    "imageio.mimsave('ex7_sgd.gif', frames, fps=2)  # You can adjust fps for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to display the GIF in a Jupyter notebook (optional):\n",
    "from IPython.display import Image\n",
    "Image(filename='ex7_sgd.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Parallelism with `pmap`\n",
    "- **Objective:** Leverage data parallelism to scale computations across multiple devices.\n",
    "- **Tasks:**\n",
    "  - Implement a function to compute across a large array. It should be complicated enough to notice a difference between a jitted version and a non-jitted version  - Use `jax.pmap` to parallelize this operation across multiple devices (e.g., GPUs).\n",
    "  - Measure the speedup obtained with `pmap` compared to single-device execution.\n",
    "  - Experiment with different batch sizes and data partitioning strategies.\n",
    "  - Try doing this in [Google Colab](https://colab.research.google.com) for free and easy access to multiple accelerator devices.\n",
    "  \n",
    "Note: `jax.pmap` compiles the function so `jit()` is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: JaxPR\n",
    "- **Objective:** Dive into Jax's internal representation by manipulating JaxPR.\n",
    "- **Tasks:**\n",
    "  - Define a function and obtain it's jaxpr using `jax.make_jaxpr`.\n",
    "  - Analyze the jaxpr to understand it's computation graph. Compare it with the one from the blog post.\n",
    "  - Modify the function and observe how the jaxpr changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
